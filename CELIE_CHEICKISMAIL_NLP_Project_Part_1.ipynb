{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6757ba7c",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning for NLP and Text Processing\n",
    "## Project 1 : OpenFoodFacts\n",
    "\n",
    "\n",
    "### Part 1 : Define and clean the vocabulary of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06796349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "# !pip install chardet\n",
    "# !pip install python-magic\n",
    "# !pip install pyenchant\n",
    "# !pip install hunspell \n",
    "# !pip install tensorflow\n",
    "# !pip install nltk\n",
    "# !pip install langdetect\n",
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcd7f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\cheic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import librairies\n",
    "\n",
    "# import enchant\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.metrics import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2d7e4",
   "metadata": {},
   "source": [
    "### Load clean and export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9667acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language used in ingredients and keep only those in english\n",
    "count = 1\n",
    "\n",
    "def define_language(x): \n",
    "    global count\n",
    "    x = x.lower()\n",
    "    \n",
    "#     with open('./logs/all_ingredients.txt', 'a') as f:\n",
    "#         print(f\"Ingredients n°{count} : {x}\", file=f)\n",
    "#         print('-'*30, file=f)\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if re.findall(\"^[a-z]\", x): \n",
    "        return \"en\" if detect(x) == \"en\" else None\n",
    "    \n",
    "#     else: \n",
    "# #         with open('./logs/errors.txt', 'a') as f:\n",
    "# #             print(f\"Ingredients n°{count} : {x}\", file=f)\n",
    "# #             print('-'*30, file=f)\n",
    "            \n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(PATH): \n",
    "    dataset = pd.read_csv(PATH, sep = '\\t', encoding='latin1') \n",
    "    start_time = time.time()\n",
    "\n",
    "    columns_to_drop = [\n",
    "        'url', \n",
    "        'code',\n",
    "        'creator',\n",
    "        'created_t',\n",
    "        'created_datetime',\n",
    "        'last_modified_t',\n",
    "        'last_modified_datetime',\n",
    "        'abbreviated_product_name',\n",
    "        'generic_name',\n",
    "        'packaging',\n",
    "        'packaging_tags',\n",
    "        'packaging_text',\n",
    "        'brands',\n",
    "        'categories',\n",
    "        'categories_en',\n",
    "        'origins',\n",
    "        'origins_en',\n",
    "        'manufacturing_places',\n",
    "        'labels',\n",
    "        'labels_en',\n",
    "        'emb_codes',\n",
    "        'emb_codes_tags',\n",
    "        'first_packaging_code_geo',\n",
    "        'cities',\n",
    "        'purchase_places',\n",
    "        'stores',\n",
    "        'countries',\n",
    "        'countries_en',\n",
    "        'traces',\n",
    "        'traces_en',\n",
    "        'allergens_en',\n",
    "        'serving_size',\n",
    "        'serving_quantity',\n",
    "        'additives',\n",
    "        'additives_en',\n",
    "        'ingredients_from_palm_oil',\n",
    "        'ingredients_that_may_be_from_palm_oil',\n",
    "        'states',\n",
    "        'states_tags',\n",
    "        'states_en',\n",
    "        'main_category_en',\n",
    "        'image_small_url',\n",
    "        'image_ingredients_url',\n",
    "        'image_ingredients_small_url',\n",
    "        'image_nutrition_url',\n",
    "        'image_nutrition_small_url'\n",
    "    ]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = dataset.drop(columns = columns_to_drop)\n",
    "\n",
    "    # Drop rows where product_name, categories_tags or ingredients_text are empty\n",
    "    df = df.dropna(subset = ['product_name', 'categories_tags', 'ingredients_text'])\n",
    "\n",
    "    # Detect language and keep only those in english\n",
    "    df[\"ingredients_text_language\"] = df[\"ingredients_text\"].apply(define_language) \n",
    "    df = df.dropna(subset = ['ingredients_text_language'])\n",
    "    \n",
    "    # Then drop created column as not necessary for the next steps\n",
    "    df = df.drop(columns = ['ingredients_text_language'])\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"PATH : {PATH} -- Execution Time : {end_time - start_time}\")\n",
    "    print(\"=\"*10)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8156b6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b25fb31f7600>:5: DtypeWarning: Columns (1,14,28,29,30,32,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  splitted_datasets.append(clean_dataset(PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH : ./datasets/openfoodfacts_part1.csv -- Execution Time : 342.2369873523712\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b25fb31f7600>:5: DtypeWarning: Columns (1,14,30,32,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  splitted_datasets.append(clean_dataset(PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH : ./datasets/openfoodfacts_part2.csv -- Execution Time : 372.180499792099\n",
      "==========\n",
      "PATH : ./datasets/openfoodfacts_part3.csv -- Execution Time : 386.99606108665466\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b25fb31f7600>:5: DtypeWarning: Columns (1,14,23,24,28,29,30,32,41,42,43,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  splitted_datasets.append(clean_dataset(PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH : ./datasets/openfoodfacts_part4.csv -- Execution Time : 198.53735995292664\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b25fb31f7600>:5: DtypeWarning: Columns (14,28,29,30,32,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  splitted_datasets.append(clean_dataset(PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH : ./datasets/openfoodfacts_part5.csv -- Execution Time : 390.5307605266571\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b25fb31f7600>:5: DtypeWarning: Columns (1,14,23,24,30,32,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  splitted_datasets.append(clean_dataset(PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH : ./datasets/openfoodfacts_part6.csv -- Execution Time : 307.17069268226624\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b25fb31f7600>:5: DtypeWarning: Columns (1,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  splitted_datasets.append(clean_dataset(PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH : ./datasets/openfoodfacts_part7.csv -- Execution Time : 52.36095666885376\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b25fb31f7600>:5: DtypeWarning: Columns (1,14,28,29,30,32,41,42,43,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  splitted_datasets.append(clean_dataset(PATH))\n"
     ]
    },
    {
     "ename": "LangDetectException",
     "evalue": "No features in text.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLangDetectException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b25fb31f7600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m41\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./datasets/openfoodfacts_part'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msplitted_datasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitted_datasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6f6f19b92ca5>\u001b[0m in \u001b[0;36mclean_dataset\u001b[1;34m(PATH)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# Detect language and keep only those in english\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ingredients_text_language\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ingredients_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefine_language\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ingredients_text_language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e2f4e3a64472>\u001b[0m in \u001b[0;36mdefine_language\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"^[a-z]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"en\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"en\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#     else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\langdetect\\detector_factory.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_factory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\langdetect\\detector.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mwhich\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhighest\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         '''\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\langdetect\\detector.py\u001b[0m in \u001b[0;36mget_probabilities\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlangprob\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_detect_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlangprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\langdetect\\detector.py\u001b[0m in \u001b[0;36m_detect_block\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mngrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mLangDetectException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCantDetectError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'No features in text.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlangprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanglist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLangDetectException\u001b[0m: No features in text."
     ]
    }
   ],
   "source": [
    "splitted_datasets = []\n",
    "\n",
    "for i in range(1,41): \n",
    "    PATH = './datasets/openfoodfacts_part'+ str(i) +'.csv'\n",
    "    splitted_datasets.append(clean_dataset(PATH))\n",
    "\n",
    "dataset = pd.concat(splitted_datasets)\n",
    "\n",
    "# Save cleaned dataset to make the next steps easier\n",
    "dataset.to_csv('./datasets/clean_openfoodfacts.csv', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a90b22",
   "metadata": {},
   "source": [
    "### Tokenize ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[A-Za-z'%-]+\")\n",
    "dataset[\"ingredients\"] = dataset[\"ingredients_text\"].apply(\n",
    "    lambda x : tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ca1d9",
   "metadata": {},
   "source": [
    "### Handle mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a698bcf",
   "metadata": {},
   "source": [
    "#### First method : using NLTK's corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "set_ingredients = set([_.lower() for list in dataset[\"ingredients\"].to_list() for _ in list])\n",
    "\n",
    "for word in list(set_ingredients):\n",
    "    list_distance = list()\n",
    "    for _ in english_vocab:\n",
    "        list_distance.append(edit_distance(_, word))\n",
    "    print(f\"{word} ==> {list(english_vocab)[list_distance.index(min(list_distance))]}\", flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c700a77",
   "metadata": {},
   "source": [
    "#### Second method : using SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "start_time = time.time()\n",
    "for _ in set_ingredients:\n",
    "    misspelled = spell.unknown([_])\n",
    "    if len(misspelled):\n",
    "        print(f\"{_} ==> {spell.correction(list(misspelled)[0])}\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution Time : {end_time - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
