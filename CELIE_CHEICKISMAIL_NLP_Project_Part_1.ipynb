{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6757ba7c",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning for NLP and Text Processing\n",
    "## Project 1 : OpenFoodFacts\n",
    "\n",
    "\n",
    "### Part 1 : Define and clean the vocabulary of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06796349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "# !pip install chardet\n",
    "# !pip install python-magic\n",
    "# !pip install pyenchant\n",
    "# !pip install hunspell \n",
    "# !pip install tensorflow\n",
    "# !pip install nltk\n",
    "# !pip install langdetect\n",
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcd7f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\cheic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import librairies\n",
    "\n",
    "# import enchant\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.metrics import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2d7e4",
   "metadata": {},
   "source": [
    "### Load clean and export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9667acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language used in ingredients and keep only those in english\n",
    "count = 1\n",
    "\n",
    "def define_language(x): \n",
    "    global count\n",
    "    x = x.lower()\n",
    "    \n",
    "    with open('./logs/all_ingredients.txt', 'a') as f:\n",
    "        print(f\"Ingredients n°{count} : {x}\", file=f)\n",
    "        print('-'*30, file=f)\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if re.findall(\"^[a-z]\", x): \n",
    "        return \"en\" if detect(x) == \"en\" else None\n",
    "    \n",
    "    else: \n",
    "        with open('./logs/errors.txt', 'a') as f:\n",
    "            print(f\"Ingredients n°{count} : {x}\", file=f)\n",
    "            print('-'*30, file=f)\n",
    "            \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(PATH): \n",
    "    dataset = pd.read_csv(PATH, sep = '\\t', encoding='latin1') \n",
    "    start_time = time.time()\n",
    "\n",
    "    columns_to_drop = [\n",
    "        'url', \n",
    "        'code',\n",
    "        'creator',\n",
    "        'created_t',\n",
    "        'created_datetime',\n",
    "        'last_modified_t',\n",
    "        'last_modified_datetime',\n",
    "        'abbreviated_product_name',\n",
    "        'generic_name',\n",
    "        'packaging',\n",
    "        'packaging_tags',\n",
    "        'packaging_text',\n",
    "        'brands',\n",
    "        'categories',\n",
    "        'categories_en',\n",
    "        'origins',\n",
    "        'origins_en',\n",
    "        'manufacturing_places',\n",
    "        'labels',\n",
    "        'labels_en',\n",
    "        'emb_codes',\n",
    "        'emb_codes_tags',\n",
    "        'first_packaging_code_geo',\n",
    "        'cities',\n",
    "        'purchase_places',\n",
    "        'stores',\n",
    "        'countries',\n",
    "        'countries_en',\n",
    "        'traces',\n",
    "        'traces_en',\n",
    "        'allergens_en',\n",
    "        'serving_size',\n",
    "        'serving_quantity',\n",
    "        'additives',\n",
    "        'additives_en',\n",
    "        'ingredients_from_palm_oil',\n",
    "        'ingredients_that_may_be_from_palm_oil',\n",
    "        'states',\n",
    "        'states_tags',\n",
    "        'states_en',\n",
    "        'main_category_en',\n",
    "        'image_small_url',\n",
    "        'image_ingredients_url',\n",
    "        'image_ingredients_small_url',\n",
    "        'image_nutrition_url',\n",
    "        'image_nutrition_small_url'\n",
    "    ]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = dataset.drop(columns = columns_to_drop)\n",
    "\n",
    "    # Drop rows where product_name, categories_tags or ingredients_text are empty\n",
    "    df = df.dropna(subset = ['product_name', 'categories_tags', 'ingredients_text'])\n",
    "\n",
    "    # Detect language and keep only those in english\n",
    "    df[\"ingredients_text_language\"] = df[\"ingredients_text\"].apply(define_language) \n",
    "    df = df.dropna(subset = ['ingredients_text_language'])\n",
    "    \n",
    "    # Then drop created column as not necessary for the next steps\n",
    "    df = dataset.drop(columns = ['ingredients_text_language'])\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"PATH : {PATH} -- Execution Time : {end_time - start_time}\")\n",
    "    print(\"=\"*10)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc58284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-7cb49c4bb857>:2: DtypeWarning: Columns (1,14,28,29,30,32,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  df = clean_dataset(PATH)\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\x89' in position 71: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7cb49c4bb857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./datasets/openfoodfacts_part1.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f305b7f83c3a>\u001b[0m in \u001b[0;36mclean_dataset\u001b[1;34m(PATH)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# Detect language and keep only those in english\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ingredients_text_language\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ingredients_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefine_language\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ingredients_text_language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-60bd1a735025>\u001b[0m in \u001b[0;36mdefine_language\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./logs/all_ingredients.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Ingredients n°{count} : {x}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python\\python38\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\x89' in position 71: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "PATH = './datasets/openfoodfacts_part1.csv'\n",
    "df = clean_dataset(PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_datasets = []\n",
    "\n",
    "for i in range(1,41): \n",
    "    PATH = './datasets/openfoodfacts_part'+ str(i) +'.csv'\n",
    "    try: \n",
    "        df = clean_dataset(PATH)\n",
    "    except: \n",
    "        df = clean_dataset(PATH)\n",
    "        with open('./logs/encoding_error.txt', 'a') as f:\n",
    "            print(f\"Encoding error in file : {PATH}\", file=f)\n",
    "\n",
    "dataset = pd.concat(splitted_datasets)\n",
    "\n",
    "# Save cleaned dataset to make the next steps easier\n",
    "dataset.to_csv('./datasets/clean_openfoodfacts.csv', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a90b22",
   "metadata": {},
   "source": [
    "### Tokenize ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[A-Za-z'%-]+\")\n",
    "dataset[\"ingredients\"] = dataset[\"ingredients_text\"].apply(\n",
    "    lambda x : tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ca1d9",
   "metadata": {},
   "source": [
    "### Handle mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a698bcf",
   "metadata": {},
   "source": [
    "#### First method : using NLTK's corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "set_ingredients = set([_.lower() for list in dataset[\"ingredients\"].to_list() for _ in list])\n",
    "\n",
    "for word in list(set_ingredients):\n",
    "    list_distance = list()\n",
    "    for _ in english_vocab:\n",
    "        list_distance.append(edit_distance(_, word))\n",
    "    print(f\"{word} ==> {list(english_vocab)[list_distance.index(min(list_distance))]}\", flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c700a77",
   "metadata": {},
   "source": [
    "#### Second method : using SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "start_time = time.time()\n",
    "for _ in set_ingredients:\n",
    "    misspelled = spell.unknown([_])\n",
    "    if len(misspelled):\n",
    "        print(f\"{_} ==> {spell.correction(list(misspelled)[0])}\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution Time : {end_time - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
