{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning for NLP and Text Processing\n",
    "## Project 1 : OpenFoodFacts\n",
    "\n",
    "\n",
    "### Part 1 : Define and clean the vocabulary of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "# !pip install pyenchant\n",
    "# !pip install hunspell \n",
    "!pip install tensorflow\n",
    "!pip install nltk\n",
    "!pip install langdetect\n",
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import librairies\n",
    "\n",
    "# import enchant\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.metrics import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "PATH = './datasets/openfoodfacts.csv'\n",
    "dataset = pd.read_csv(PATH, sep = '\\t') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'url', \n",
    "    'code',\n",
    "    'creator',\n",
    "    'created_t',\n",
    "    'created_datetime',\n",
    "    'last_modified_t',\n",
    "    'last_modified_datetime',\n",
    "    'abbreviated_product_name',\n",
    "    'generic_name',\n",
    "    'packaging',\n",
    "    'packaging_tags',\n",
    "    'packaging_text',\n",
    "    'brands',\n",
    "    'categories',\n",
    "    'categories_en',\n",
    "    'origins',\n",
    "    'origins_en',\n",
    "    'manufacturing_places',\n",
    "    'labels',\n",
    "    'labels_en',\n",
    "    'emb_codes',\n",
    "    'emb_codes_tags',\n",
    "    'first_packaging_code_geo',\n",
    "    'cities',\n",
    "    'purchase_places',\n",
    "    'stores',\n",
    "    'countries',\n",
    "    'countries_en',\n",
    "    'traces',\n",
    "    'traces_en',\n",
    "    'allergens_en',\n",
    "    'serving_size',\n",
    "    'serving_quantity',\n",
    "    'additives',\n",
    "    'additives_en',\n",
    "    'ingredients_from_palm_oil',\n",
    "    'ingredients_that_may_be_from_palm_oil',\n",
    "    'states',\n",
    "    'states_tags',\n",
    "    'states_en',\n",
    "    'main_category_en',\n",
    "    'image_small_url',\n",
    "    'image_ingredients_url',\n",
    "    'image_ingredients_small_url',\n",
    "    'image_nutrition_url',\n",
    "    'image_nutrition_small_url'\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = dataset.drop(columns = columns_to_drop)\n",
    "\n",
    "# Drop rows where product_name; categories_tags or ingredients_text are empty\n",
    "df = df.dropna(subset = ['product_name', 'categories_tags', 'ingredients_text'])\n",
    "\n",
    "# Detect language used in ingredients and keep only those in english\n",
    "df[\"ingredients_text_language\"] = df[\"ingredients_text\"].apply(\n",
    "    lambda x : \"en\" if detect(x.lower()) == \"en\" else None) \n",
    "df = df.dropna(subset = ['ingredients_text_language'])\n",
    "\n",
    "# Then drop created column as not necessary for the next steps\n",
    "df = dataset.drop(columns = ['ingredients_text_language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset to make the next steps easier\n",
    "df.to_csv('./datasets/clean_openfoodfacts.csv', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[A-Za-z'%-]+\")\n",
    "df[\"ingredients\"] = df[\"ingredients_text\"].apply(\n",
    "    lambda x : tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First method : using NLTK's corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "set_ingredients = set([_.lower() for list in df[\"ingredients\"].to_list() for _ in list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in list(set_ingredients):\n",
    "    list_distance = list()\n",
    "    for _ in english_vocab:\n",
    "        list_distance.append(edit_distance(_, word))\n",
    "    print(f\"{word} ==> {list(english_vocab)[list_distance.index(min(list_distance))]}\", flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second method : using SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "for _ in set_ingredients:\n",
    "    misspelled = spell.unknown([_])\n",
    "    if len(misspelled):\n",
    "        print(f\"{_} ==> {spell.correction(list(misspelled)[0])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
