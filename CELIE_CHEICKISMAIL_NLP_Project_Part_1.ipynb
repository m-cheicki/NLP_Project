{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6757ba7c",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning for NLP and Text Processing\n",
    "## Project 1 : OpenFoodFacts\n",
    "\n",
    "\n",
    "### Part 1 : Define and clean the vocabulary of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06796349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "# !pip install chardet\n",
    "# !pip install python-magic\n",
    "# !pip install pyenchant\n",
    "# !pip install hunspell \n",
    "# !pip install tensorflow\n",
    "# !pip install nltk\n",
    "# !pip install langdetect\n",
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcd7f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\cheic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import librairies\n",
    "\n",
    "# import enchant\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.metrics import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2d7e4",
   "metadata": {},
   "source": [
    "### Load clean and export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9667acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language used in ingredients and keep only those in english\n",
    "def define_language(x): \n",
    "    global count\n",
    "    x = x.lower()\n",
    "    if re.findall(\"^[a-z]\", x): \n",
    "        try: \n",
    "            lang = detect(x)\n",
    "            return \"en\" if lang == \"en\" else None\n",
    "        except : \n",
    "            print(\"Invalid character : \" + x)\n",
    "            return None\n",
    "    else: \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(PATH): \n",
    "    dataset = pd.read_csv(PATH, sep = '\\t', encoding='latin1') \n",
    "    start_time = time.time()\n",
    "\n",
    "    columns_to_drop = [\n",
    "        'url', \n",
    "        'code',\n",
    "        'creator',\n",
    "        'created_t',\n",
    "        'created_datetime',\n",
    "        'last_modified_t',\n",
    "        'last_modified_datetime',\n",
    "        'abbreviated_product_name',\n",
    "        'generic_name',\n",
    "        'packaging',\n",
    "        'packaging_tags',\n",
    "        'packaging_text',\n",
    "        'brands',\n",
    "        'categories',\n",
    "        'categories_en',\n",
    "        'origins',\n",
    "        'origins_en',\n",
    "        'manufacturing_places',\n",
    "        'labels',\n",
    "        'labels_en',\n",
    "        'emb_codes',\n",
    "        'emb_codes_tags',\n",
    "        'first_packaging_code_geo',\n",
    "        'cities',\n",
    "        'purchase_places',\n",
    "        'stores',\n",
    "        'countries',\n",
    "        'countries_en',\n",
    "        'traces',\n",
    "        'traces_en',\n",
    "        'allergens_en',\n",
    "        'serving_size',\n",
    "        'serving_quantity',\n",
    "        'additives',\n",
    "        'additives_en',\n",
    "        'ingredients_from_palm_oil',\n",
    "        'ingredients_that_may_be_from_palm_oil',\n",
    "        'states',\n",
    "        'states_tags',\n",
    "        'states_en',\n",
    "        'main_category_en',\n",
    "        'image_small_url',\n",
    "        'image_ingredients_url',\n",
    "        'image_ingredients_small_url',\n",
    "        'image_nutrition_url',\n",
    "        'image_nutrition_small_url'\n",
    "    ]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = dataset.drop(columns = columns_to_drop)\n",
    "\n",
    "    # Drop rows where product_name, categories_tags or ingredients_text are empty\n",
    "    df = df.dropna(subset = ['product_name', 'categories_tags', 'ingredients_text'])\n",
    "\n",
    "    # Detect language and keep only those in english\n",
    "    df[\"ingredients_text_language\"] = df[\"ingredients_text\"].apply(define_language) \n",
    "    df = df.dropna(subset = ['ingredients_text_language'])\n",
    "    \n",
    "    # Then drop created column as not necessary for the next steps\n",
    "    df = df.drop(columns = ['ingredients_text_language'])\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"PATH : {PATH} -- Execution Time : {end_time - start_time}\")\n",
    "    print(\"=\"*10)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b96226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-00330e237e7f>:2: DtypeWarning: Columns (0,8,13,19,20,21,22,23,27,28,29,31,52,55,64) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  openfoodfacts = clean_dataset(PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid character : https://vm.tiktok.com/zme7qpxg5/\n",
      "Invalid character : https://hubpak.com/ https://www.eshop.hubpak.com/\n",
      "Invalid character : https://static.openfoodfacts.org/images/products/325/039/128/5556/4.100.jpg\n",
      "PATH : ./datasets/openfoodfacts.csv -- Execution Time : 6131.624509572983\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "PATH = './datasets/openfoodfacts.csv'\n",
    "openfoodfacts = clean_dataset(PATH)\n",
    "openfoodfacts.shape\n",
    "openfoodfacts.to_csv('./datasets/openfoodfacts_clean.csv', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd356bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "openfoodfacts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a90b22",
   "metadata": {},
   "source": [
    "### Tokenize ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[A-Za-z'%-]+\")\n",
    "dataset[\"ingredients\"] = dataset[\"ingredients_text\"].apply(\n",
    "    lambda x : tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ca1d9",
   "metadata": {},
   "source": [
    "### Handle mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a698bcf",
   "metadata": {},
   "source": [
    "#### First method : using NLTK's corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "set_ingredients = set([_.lower() for list in dataset[\"ingredients\"].to_list() for _ in list])\n",
    "\n",
    "for word in list(set_ingredients):\n",
    "    list_distance = list()\n",
    "    for _ in english_vocab:\n",
    "        list_distance.append(edit_distance(_, word))\n",
    "    print(f\"{word} ==> {list(english_vocab)[list_distance.index(min(list_distance))]}\", flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c700a77",
   "metadata": {},
   "source": [
    "#### Second method : using SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "start_time = time.time()\n",
    "for _ in set_ingredients:\n",
    "    misspelled = spell.unknown([_])\n",
    "    if len(misspelled):\n",
    "        print(f\"{_} ==> {spell.correction(list(misspelled)[0])}\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution Time : {end_time - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
